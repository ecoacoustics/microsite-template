{{ $audio_recording_id := .Get "audioRecordingId" }}
{{ $audio_event_id := .Get "audioEventId" }}
{{ $label := .Get "label" }}

{{- if not $audio_recording_id -}}
    {{ errorf "Missing audioRecordingId for event card with label %s" $label }}
{{- end -}}

{{- if not $audio_event_id -}}
    {{ errorf "Missing audioEventId for event card with label %s" $label }}
{{- end -}}

{{ $id := printf "event-card-%03d" $.Ordinal }}

<div id="{{ $id }}" class="card-host">
    <sl-card class="spectrogram-card">
        <div class="card-header" slot="header">
            <h3 class="event-label">{{ $label }}</h3>
            <oe-media-controls for="{{ $id }}-spectrogram"></oe-media-controls>
        </div>

        <div class="card-body">
            <oe-annotate id="{{ $id }}-annotations">
                <oe-axes>
                    <oe-indicator>
                        <oe-spectrogram
                            id="{{ $id }}-spectrogram"
                            class="event-spectrogram"
                        ></oe-spectrogram>
                    </oe-indicator>
                </oe-axes>
            </oe-annotate>
        </div>

        {{ .Inner }}
    </sl-card>
</div>

<script type="module">
    const spectrogramTarget = document.getElementById("{{ $id }}-spectrogram");
    const annotationTarget = document.getElementById("{{ $id }}-annotations");

    // TODO: Refactor this function once we support object annotations
    // see: https://github.com/ecoacoustics/web-components/issues/501
    function createAnnotation(audioEvent, label, contextPaddingStart) {
        const eventDuration = audioEvent.end_time_seconds - audioEvent.start_time_seconds;

        const annotationElement = document.createElement("oe-annotation");
        annotationElement.setAttribute("start-time", contextPaddingStart);
        annotationElement.setAttribute("end-time", eventDuration + contextPaddingStart);
        annotationElement.setAttribute("low-frequency", audioEvent.low_frequency_hertz);
        annotationElement.setAttribute("high-frequency", audioEvent.high_frequency_hertz);
        annotationElement.setAttribute("tags", label);

        annotationTarget.appendChild(annotationElement);
    }

    const api = await workbenchApi();

    const audioRecordingId = {{ $audio_recording_id }};
    const audioEventId = {{ $audio_event_id }};
    const label = "{{ $label }}";

    // We use promise.all here so that we can fetch both the audio event and the
    // audio recording in parallel.
    // We also use .all instead of .allSettled so that if one of the requests
    // fails, we can immediately show an error to the user instead of waiting
    // for all requests to finish.
    const [audioRecording, audioEvent] = await Promise.all([
        api.getAudioRecording(audioRecordingId),
        api.getAudioEvent(audioRecordingId, audioEventId),
    ]);

    const eventDuration = audioEvent.end_time_seconds - audioEvent.start_time_seconds;
    const audioRecordingDuration = audioRecording.duration_seconds;

    // A padding around the event in seconds to give the user some context
    // If the event is very short (e.g. less than 8 seconds), we we want to use
    // some smaller context padding so that the event is the center of the users
    // attention.
    // But if we have a longer event, we want to fix the context padding to a
    // maximum of 2 seconds in order to conserve visual space.
    //
    // Additionally, we clamp to the start and end of the recording so that we
    // don't try to fetch audio that is outside the bounds of the recording.
    const contextPadding = Math.min(2, eventDuration / 4);
    const contextStartTime = Math.max(audioEvent.start_time_seconds - contextPadding, 0);
    const contextEndTime = Math.min(audioEvent.end_time_seconds + contextPadding, audioRecordingDuration);

    // How much padding we added to the start of the event. Because this can be
    // clamped to the start of the recording, we need to calculate this after
    // we calculate the context start time.
    const contextPaddingStart = audioEvent.start_time_seconds - contextStartTime;

    const eventWithContext = {
        ...audioEvent,
        start_time_seconds: contextStartTime,
        end_time_seconds: contextEndTime,
    };
    spectrogramTarget.src = api.audioEventUrl(eventWithContext);

    // Draws a bounding box around the original event.
    // We need the contextPaddingStart because we need to know how many seconds
    // from the start of the spectrogram the event starts at.
    // We cannot use the contextPadding in case the context start time was
    // clamped to the start of the recording.
    createAnnotation(audioEvent, label, contextPaddingStart);
</script>

<style>
    #{{ $id }}.card-host {
        display: block;
        position: relative;

        margin-bottom: 1.5rem;
    }

    .card-header {
        display: flex;
        justify-content: space-between;
        align-items: center;

        .event-label {
            margin: 0;

            font-size: 1.2rem;
            letter-spacing: 0.5px;
        }
    }

    .spectrogram-card {
        display: block;
    }
</style>
